# Model-Based IR

## NQ 100k
### Overall Performance of Retrievers
|Method|MRR@10|Recall@5|Recall@10|Recall@100|Reacll@1000|
|:-:|:-:|:-:|:-:|:-:|:-:|
|BM25|0.467|0.6029|0.6897|0.8764|0.9618|
|DPR|0.7137|0.8419|0.886|0.9554|0.9839|
|ANCE (1 iter)|0.7319|0.8529|0.8928|0.9553|0.9822|
|UniCOIL|0.7095|0.8298|0.8715|0.9461|0.9797|
|SPLADE|0.7173|0.8398|0.8829|0.9589|0.9865|
|GENRE||0.6195|0.6997|0.7359|--|--|
|GENRE-qg|0.6718|0.7488|0.7891|--|--|
|DSI|0.4912|0.5475|0.5752|--|--|
|DSI_bias|0.5021|0.5524|0.5718|--|--|
|DSI_better-code|0.5351|0.6003|0.6344|--|--|
|DSI_better-code_bias|0.5248|0.5811|0.611|--|--|
|DSI_qg|0.6589|0.7478|0.7793|--|--|
|DSI_better-code_qg|0.6916|0.7784|0.8088|0.8787|--|

### Overall Performance of Rankers
|Method|retriever|MRR@10|Recall@5|Recall@10|Recall@100|Reacll@1000|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|ANCE (1 iter)|--|0.7319|0.8529|0.8928|0.9553|0.9822|
|DSI|--|0.5351|0.6003|0.6344|--|--|
|DSI|ANCE_top1000|0.5477|0.6184|0.6586|0.8057|0.9822|
|GENRE|ANCE_top1000|0.6473|0.7271|0.7548|0.8368|0.9822|
|CrossEncoder|ANCE_top1000|0.7925|0.9047|0.9336|0.9723|0.9822|
|CrossEncoder_title|ANCE_top1000||
|CrossEncoder_dsi-code|ANCE_top1000|0.3478|0.4501|0.5189|0.7543|0.9822|
|CrossEncoder_UniCOIL-weight|ANCE_top1000|0.8009|0.8997|0.9281|0.9718|0.9822|
|Seq2SeqRanker|ANCE_top1000|0.7836|0.8932|0.9253|0.9709|0.9822|
|Seq2SeqRanker_title|ANCE_top1000||
|Seq2SeqRanker_dsi-code|ANCE_top1000|0.3973|0.4923|0.5373|0.6955|0.9822|
|Seq2SeqRanker_UniCOIL-weight|ANCE-top1000|0.7713|0.8746|0.906|0.9605|0.9822|

### Ablation for Basic Settings
|Ranking Token|MRR@10|Recall@5|Recall@10|Recall@100|Reacll@1000|
|:-:|:-:|:-:|:-:|:-:|:-:|
|sep|0.7724|0.8722|0.9034|0.9603|0.9822|
|last|0.7713|0.8746|0.906|0.9605|0.9822|

|Ablation|Batch Size|Learning Rate|Scheduler|Loss|Hard Negative Number|Code|MRR@10|Recall@5|Recall@10|Recall@100|Reacll@1000|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|CrossEntropy|32|3e-5|--|CrossEntropy|49|UniCOIL-weight|0.7713|0.8746|0.906|0.9605|0.9822|
|BCE loss|256|3e-5|--|BCE|49|UniCOIL-weight|0.7622|0.8553|0.8902|0.9521|0.9822|
|BCE; w. larger batch size|512|3e-5|--|BCE|49|UniCOIL-weight|
|BCE; w. larger batch size; w. more negatives|512|3e-5|--|BCE|99|UniCOIL-weight|0.7709|0.8665|0.8997|0.9599|0.9822|

### Beam Search
|Method|Candidate Set|Code|Training|Beam Size|Ranking Token|MRR@10|Recall@5|Recall@10|Recall@100|Reacll@1000|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|ANCE|--|--|--|--|--|0.7319|0.8529|0.8928|0.9553|0.9822|
|Seq2Seq|ANCE_top1000|UniCOIL-weight|sequence level|--|--|0.7713|0.8746|0.906|0.9605|0.9822|
|Seq2Seq|ANCE_top1000|UniCOIL-weight|token level|--|--|0.7686|0.8699|0.9005|0.9548|0.9822|
|Seq2Seq|ANCE_top1000|UniCOIL-weight|sequence level|100|fewest|0.6109|0.7163|0.755|0.8041|--|
|Seq2Seq|ANCE_top1000|UniCOIL-weight|token level|100|fewest|0.689|0.7917|0.824|0.8672|--|
|Seq2Seq|ANCE_top1000|UniCOIL-weight|token level|100|last|0.7339|0.8209|0.8434|0.8672|--|
|Seq2Seq|--|UniCOIL-weight|token level|100|fewest|0.6556|0.7397|0.7563|0.7787|--|
|Seq2Seq|ANCE_top1000|UniCOIL-weight|token level|500,400,300,200,100|fewest|0.7024|0.807|0.843|0.8982|--|
|Seq2Seq|ANCE_top1000|UniCOIL-weight|token level|500,400,300,200,100|last|0.7488|0.8436|0.867|0.8982|--|
|Seq2Seq; w.t. lsh restore|ANCE_top1000|UniCOIL-weight|token level|100|last|0.742|0.8324|0.8575|0.8883|0.8926|



### Instructions
```python
# hierarchical cluster text embeddings
python run.py -cn dpr base=NQ +mode=cluster model.verbose=hn model.cluster_type=hier

# run GENRE
python run.py -cn dsi model.code_type=title model.code_length=34
# run DSI with k=10, c=10
python run.py -cn dsi +code_size=10
# run DSI over candidates from DPR_hn
python run.py -cn dsi +mode=rerank +code_size=10 eval=rerank +candidate_type=DPR_hn

# train crossencoder by cross-entropy
python run.py -cn crossenc base=NQ train.hard_neg_type=DPR_hn +candidate_type=DPR_hn
# train crossencoder with document code as the document content
python run.py -cn crossenc +hard_neg_type=DPR +hard_neg_num=49 +batch_size=14 +candidate_type=DPR +return_code=true +code_type=UniCOIL-weight +code_length=34 +code_tokenizer=bert +world_size=2
# train seq2seqranker with bce loss
python run.py -cn seq2seq +candidate_type=DPR_hn +batch_size=64 +code_type=UniCOIL-weight +code_length=34 +return_code=true +code_tokenizer=t5 +world_size=4
# train seq2seqranker with cross-entropy loss
python run.py -cn seq2seq train=neg +hard_neg_type=DPR_hn +hard_neg_num=49 +batch_size=16 +candidate_type=DPR_hn +code_type=UniCOIL-weight +code_length=34 +code_tokenizer=t5 +return_code=true +world_size=2
# train seq2seqranker with cross-entropy and use all token ranking
python run.py -cn seq2seq train=neg +hard_neg_type=DPR_hn +hard_neg_num=49 +batch_size=8 +return_prefix_mask=true +candidate_type=DPR_hn +code_type=UniCOIL-weight +code_length=34 +code_tokenizer=t5 +return_code=true +nbeam=100 ranking_token=all beam_ranking=fewest
# eval by retrieval
python run.py -cn seq2seq +candidate_type=DPR_hn +code_type=UniCOIL-weight +code_length=34 +code_tokenizer=t5 +return_code=true +nbeam=100 +batch_size_eval=1 eval=retrieve ranking_token=all beam_ranking=fewest +mode=eval
# UniCOIL
python run.py -cn unicoil base.world_size=2 +batch_size=5 train.accumulate_step=4
# SPLADE
python run.py -cn spladev2 base=NQ base.world_size=2 train=triple +batch_size=16 train.eval_step=0.5e train.hold_step=0 ~train.total_step

# UniCOIL generate word weights
python run.py -cn unicoil base=NQ index=anserini +index_type=impact +device=cpu +mode=index +load_cache=true +quantize_bits=0
# re-tokenize the words by some order (specified after the hyphen in code_type)
python run.py -cn unicoil base=NQ +mode=code +code_type=UniCOIL-weight +code_length=34 +code_tokenizer=t5 +device=cpu
```

### Tips
- essential to add eos_token after each code, otherwise the model is learning to generate a sequence of the length `code_length`
