loader_train: none

epoch: 20
batch_size: 32

main_metric: MRR@10
# interval of testing the model performance
eval_step: "1e"
# donot test the model performance before eval_delay steps
eval_delay: 0
# if true, save the model after validation
# otherwise, only store the ever-best performance model
save_at_eval: False
# if the metric on dev set is worse than the best record for x times, stop training
# early_stop_count: 5
# gradient accumulation
accumulate_step: 1

# mixed precision
fp16: false

learning_rate: 3e-6
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-8
weight_decay: 0.01

scheduler: constant
warmup_ratio: 0.1
warmup_steps: 0
max_grad_norm: 0

# training objective
objective: contra
distill_src: none
distill_type: bi

enable_inbatch_negative: True
# if true, gather all the embeddings across processes in distributed training
enable_all_gather: True