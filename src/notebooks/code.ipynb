{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/peitian/Envs/adon/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-09 06:36:33,593] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-09 06:36:35,968] INFO (Config) setting seed to 42...\n",
      "[2023-10-09 06:36:35,974] INFO (Config) setting PLM to t5...\n",
      "[2023-10-09 06:36:36,150] INFO (Config) Config: {'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'batch_size': 2, 'bf16': False, 'cache_root': 'data/cache/NQ320k', 'data_format': 'memmap', 'data_root': '/share/peitian/Data/AutoTSG', 'dataset': 'NQ320k', 'debug': False, 'deepspeed': None, 'device': 0, 'distill_src': 'none', 'early_stop_patience': 5, 'enable_all_gather': True, 'enable_distill': False, 'enable_inbatch_negative': True, 'epoch': 20, 'eval_batch_size': 2, 'eval_delay': 0, 'eval_flops': False, 'eval_metric': ['mrr', 'recall'], 'eval_metric_cutoff': [1, 5, 10, 100, 1000], 'eval_mode': 'retrieve', 'eval_posting_length': False, 'eval_set': 'dev', 'eval_step': '1e', 'fp16': False, 'grad_accum_step': 1, 'hits': 1000, 'index_shard': 32, 'index_thread': 10, 'index_type': 'invvec', 'learning_rate': 3e-06, 'load_ckpt': None, 'load_encode': False, 'load_index': True, 'load_query_encode': False, 'load_result': False, 'load_text_encode': False, 'loader_train': 'neg', 'main_metric': 'Recall@10', 'max_grad_norm': 0, 'max_query_length': 64, 'max_step': 0, 'max_text_length': 512, 'mode': 'train', 'model_type': None, 'neg_type': 'random', 'nneg': 1, 'num_worker': 0, 'parallel': 'text', 'plm': 't5', 'plm_dir': '/share/peitian/Data/AutoTSG/PLMs/t5', 'plm_root': '/share/peitian/Data/AutoTSG/PLMs', 'plm_tokenizer': 't5', 'posting_prune': 0.0, 'query_gate_k': 0, 'query_length': 32, 'report_to': 'none', 'return_first_mask': False, 'return_special_mask': False, 'save_at_eval': False, 'save_ckpt': 'best', 'save_encode': False, 'save_index': True, 'save_model': False, 'save_res': 'retrieval_result', 'save_score': False, 'scheduler': 'constant', 'seed': 42, 'special_token_ids': {'cls': (None, None), 'pad': ('<pad>', 0), 'unk': ('<unk>', 2), 'sep': (None, None), 'eos': ('</s>', 1)}, 'text_col': [1, 2, 3], 'text_col_sep': ' ', 'text_gate_k': 0, 'text_length': 512, 'text_type': 'default', 'train_set': ['train'], 'untie_encoder': False, 'verifier_hits': 1000, 'verifier_index': 'none', 'verifier_src': 'none', 'verifier_type': 'none', 'vocab_size': 32100, 'warmup_ratio': 0.1, 'warmup_step': 0, 'weight_decay': 0.01}\n",
      "[2023-10-09 06:36:36,190] INFO (Dataset) initializing NQ320k memmap Text dataset...\n",
      "[2023-10-09 06:36:36,232] INFO (Dataset) initializing NQ320k memmap Query dev dataset...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "if sys.path[-1] != \"../\":\n",
    "    sys.path.append(\"../\")\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "from IPython.display import display\n",
    "from random import sample\n",
    "from transformers import AutoModel, AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from models.AutoModel import AutoModel as AM\n",
    "from utils.util import *\n",
    "from utils.index import *\n",
    "from utils.data import *\n",
    "\n",
    "from hydra import initialize, compose\n",
    "\n",
    "config = Config()\n",
    "with initialize(version_base=None, config_path=\"../data/config/\"):\n",
    "    overrides = [\n",
    "        \"base=NQ320k\",\n",
    "        # \"base=MS300k\",\n",
    "        # \"++plm=t5\",\n",
    "    ]\n",
    "    hydra_config = compose(config_name=\"_example\", overrides=overrides)\n",
    "    config._from_hydra(hydra_config)\n",
    "\n",
    "loaders = prepare_data(config)\n",
    "\n",
    "loader_text = loaders[\"text\"]\n",
    "loader_query = loaders[\"query\"]\n",
    "text_dataset = loader_text.dataset\n",
    "query_dataset = loader_query.dataset\n",
    "\n",
    "# train_dataset = prepare_train_data(config, loader_text.dataset)\n",
    "# train_query_dataset = train_dataset.query_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_type = \"term\"\n",
    "# code_type = \"2gram\"\n",
    "# code_type = \"id\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 34\n",
    "\n",
    "t = AutoTokenizer.from_pretrained(os.path.join(config.plm_root, code_tokenizer))\n",
    "\n",
    "text_codes = np.memmap(\n",
    "    f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\",\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(len(text_dataset), -1).copy()\n",
    "\n",
    "# trie = TrieIndex(save_dir=f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}\")\n",
    "# trie.load()\n",
    "\n",
    "# wordset = WordSetIndex(save_dir=f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}\", sep_token_id=6)\n",
    "# wordset.fit(None)\n",
    "\n",
    "df = pd.DataFrame(text_codes)\n",
    "duplicates = df.groupby(df.columns.tolist(),as_index=False).size()\n",
    "duplicates = duplicates.sort_values(\"size\", ascending=False)\n",
    "duplicates.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dup = df.duplicated(keep=False).to_numpy()\n",
    "dup_indices = np.argwhere(dup)[:, 0]\n",
    "len(dup_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> email, marketing, mail, sending, sent, messages, advertising, campaigns, hide, customer, using, triggered, ads, online, purchase,</s><pad>']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['Email marketing Email marketing is the act of sending a commercial message, typically to a group of people, using email. In its broadest sense, every email sent to a potential or current customer could be considered email marketing. It usually involves using email to send advertisements, request business, or solicit sales or donations, and is meant to build loyalty, trust, or brand awareness. Marketing emails can be sent to a purchased lead list']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(['<pad> 1863, suspension, act, suspend, writ, civil, habeas, lincoln, proclamation, president,</s>',\n",
       "  '<pad> 1863, suspension, act, suspend, writ, civil, habeas, lincoln, proclamation, president,</s>'],\n",
       " ['Habeas corpus suspension Act 1863 The Habeas Corpus Suspension Act, 12 Stat. 755 ( 1863 ), entitled An Act',\n",
       "  'Habeas corpus suspension Act ( 1863 ) The Habeas Corpus Suspension Act, 12 Stat. 755 ( 1863 ),'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indices = random.sample(range(len(text_dataset)), 5)\n",
    "# indices = range(10)\n",
    "indices = [0]\n",
    "# indices = dup_indices\n",
    "text_code = text_codes[indices]\n",
    "text_code[text_code == -1] = 0\n",
    "# display(text_code)\n",
    "display(t.batch_decode(text_code))\n",
    "display(t.batch_decode(np.array(text_dataset[indices][\"text\"][\"input_ids\"])[:, :100]))\n",
    "\n",
    "most_dup_idx = np.argwhere((text_codes == duplicates.iloc[0].to_numpy()[:-1]).all(-1))[:, 0]\n",
    "most_dup_code = text_codes[most_dup_idx]\n",
    "most_dup_code[most_dup_code == -1] = 0\n",
    "most_dup_text = np.array(text_dataset[most_dup_idx][\"text\"][\"input_ids\"])[:, :code_length + 5]\n",
    "t.batch_decode(most_dup_code), t.batch_decode(most_dup_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check query_codes\n",
    "model = \"weight-bs200-3-greedy-sample-tau5\"\n",
    "query_sets = [\"train\"]\n",
    "dyn_text_codes = []\n",
    "qrels = []\n",
    "query_datasets = []\n",
    "for query_set in query_sets:\n",
    "    qrel = load_pickle(f\"data/cache/{config.dataset}/dataset/query/{query_set}/qrels.pkl\")\n",
    "    dyn_text_codes.append(\n",
    "        np.memmap(\n",
    "            f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/{model}/{query_set}/codes.mmp\",\n",
    "            mode=\"r\",\n",
    "            dtype=np.int32\n",
    "        ).reshape(len(qrel), -1, code_length).copy()\n",
    "    )\n",
    "    qrels.append(qrel)\n",
    "    query_datasets.append(QueryDataset(config, query_set))\n",
    "\n",
    "# text_idx: [[(qrel_idx, query_idx)], ...,]\n",
    "tidx_2_qrel_query_idx_pair = defaultdict(lambda: [[] for _ in range(len(query_sets))])\n",
    "for query_set_idx, qrel in enumerate(qrels):\n",
    "    for j, rel in enumerate(qrel):\n",
    "        tidx_2_qrel_query_idx_pair[rel[1]][query_set_idx].append((j, rel[0]))\n",
    "# new_docs = {}\n",
    "# for k,v in tidx_2_qrel_query_idx_pair.items():\n",
    "#     has_common = 0\n",
    "#     for x in v:\n",
    "#         has_common += len(x) > 0\n",
    "#     if has_common == len(query_sets):\n",
    "#         new_docs[k] = v\n",
    "# tidx_2_qrel_query_idx_pair = new_docs\n",
    "same_count = 0\n",
    "same = False\n",
    "demo = 1\n",
    "\n",
    "for tidx, qindices in tidx_2_qrel_query_idx_pair.items():\n",
    "    text_code = text_codes[tidx]\n",
    "    text_code[text_code == -1] = 0\n",
    "    # print(f'******Text Code   ({tidx}):******\\n{t.decode(text_code)}')\n",
    "\n",
    "    for query_set_idx, query_set in enumerate(query_sets):\n",
    "        query_dataset = query_datasets[query_set_idx]\n",
    "        qindice = qindices[query_set_idx]\n",
    "\n",
    "        for qrel_idx, qidx in qindice:\n",
    "            dyn_text_code = dyn_text_codes[query_set_idx][qrel_idx]\n",
    "            dyn_text_code[dyn_text_code == -1] = 0\n",
    "\n",
    "            for c in dyn_text_code:\n",
    "                if (c == text_code).all():\n",
    "                    same_count += 1\n",
    "                    same = True\n",
    "                    # if demo:\n",
    "                    #     print(f'******{query_set} Query ({qidx}):******\\n{t.decode(query_dataset[qidx][\"query\"][\"input_ids\"])}')\n",
    "                    #     print(f'******Text Code   ({tidx}):******\\n{t.decode(text_code)}')\n",
    "                    #     print(f\"******Sorted Text Code for {query_set}:******\\n{t.decode(dyn_text_code)}\")\n",
    "                else:\n",
    "                    same = False\n",
    "            if not same and demo:\n",
    "                print(f'\\n******{query_set} Query ({qidx}):******\\n{t.decode(query_dataset[qidx][\"query\"][\"input_ids\"])}')\n",
    "                print(f'******Text Code   ({tidx}):******\\n{t.decode(text_code)}')\n",
    "                print(f\"******Sorted Text Code for {query_set}:******\\n{t.batch_decode(dyn_text_code, skip_special_tokens=True)}\")\n",
    "\n",
    "                x = input()\n",
    "                if x == \"s\":\n",
    "                    raise StopIteration\n",
    "\n",
    "same_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-09 02:44:40,033] INFO (Dataset) initializing MSMARCO-passage memmap Query dev dataset...\n",
      "[2023-08-09 02:44:41,091] INFO (Dataset) initializing MSMARCO-passage memmap Query train dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Model2 wins: (259, 7067274)************\n",
      "Query                                             : ____________________ is considered the father of modern medicine.\n",
      "Target Anchor (words_comma_plus_stem)             : medicine, father, modern, hippocrates, considered, true, punishment, believe, false, weegy, illness, because,\n",
      "False Positive Anchor                             : renaissance, civilisation, rebirth, western, european, 15th, ad, key, changes, century,\n",
      "***********Model2 wins: (1523, 7067796)***********\n",
      "Query                                             : botulinum definition\n",
      "Target Anchor (words_comma_plus_stem)             : toxin, botulinum, botulism, neurotoxin, powerful, clostridium, produced, definition, medical,\n",
      "False Positive Anchor                             : fungal, toxin, fungi, toxic, fungus, weapons, botulinum, dangerous, aflatoxins, none,\n",
      "***********Model2 wins: (2319, 7067891)***********\n",
      "Query                                             : do physicians pay for insurance from their salaries?\n",
      "Target Anchor (words_comma_plus_stem)             : physicians, tricare, health, military, reserve, receive, free, participate, insurance, life, coverage, care, dependents, select,\n",
      "False Positive Anchor                             : specialty, malpractice, cost, medical, internal, average, medicine, surgeons, physicians, doctors, insurance, annual, premiums, pay, 000,\n",
      "***********Model2 wins: (2411, 7068203)***********\n",
      "Query                                             : blood diseases that are sexually transmitted\n",
      "Target Anchor (words_comma_plus_stem)             : sexually, sti, bbi, transmitted, borne, infection, blood, transmission, contact, contaminated, vaginal,\n",
      "False Positive Anchor                             : stds, stis, sexually, transmitted, diseases, infections, pass, blood, semen, vaginal, acquired,\n",
      "***********Model2 wins: (3537, 7068493)***********\n",
      "Query                                             : define bona fides\n",
      "Target Anchor (words_comma_plus_stem)             : faith, fides, bona, good, sincere, honest, fair, intention, latin, open, regardless, interactions, human, outcome,\n",
      "False Positive Anchor                             : fides, bona, bonafied, faith, latin, genuine, good, phrase, ablative, meaning, adjective, case,\n",
      "***********Model2 wins: (4700, 7068519)***********\n",
      "Query                                             : effects of detox juice cleanse\n",
      "Target Anchor (words_comma_plus_stem)             : cleanse, detox, weight, side, loss, positive, effects, unhealthy, obesity, precious, reviews, losing, severe, fat, muscle, mass,\n",
      "False Positive Anchor                             : liver, detox, help, antioxidant, olive, chronic, lemon, benefits, against, injury, oil, thanks, juice, virgin, extra, confirmed,\n"
     ]
    }
   ],
   "source": [
    "# cases that either res1 or res2 succeeds\n",
    "\n",
    "model1_result = load_pickle(f\"data/cache/MSMARCO-passage/retrieve/AutoTSG/dev/retrieval_result.pkl\")\n",
    "model2_result = load_pickle(f\"data/cache/MSMARCO-passage/retrieve/BM25/dev/retrieval_result.pkl\")\n",
    "\n",
    "code_type1 = \"words_comma_plus_stem\"\n",
    "code_type2 = \"words_comma_plus_stem\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 34\n",
    "\n",
    "text_codes1 = np.memmap(\n",
    "    f\"data/cache/{config.dataset}/codes/{code_type1}/{code_tokenizer}/{code_length}/codes.mmp\",\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(len(text_dataset), -1).copy()\n",
    "text_codes2 = np.memmap(\n",
    "    f\"data/cache/{config.dataset}/codes/{code_type2}/{code_tokenizer}/{code_length}/codes.mmp\",\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(len(text_dataset), -1).copy()\n",
    "\n",
    "t = AutoTokenizer.from_pretrained(os.path.join(config.plm_root, code_tokenizer))\n",
    "\n",
    "query_dataset = QueryDataset(config)\n",
    "\n",
    "positives = load_pickle(f\"data/cache/{config.dataset}/dataset/query/dev/positives.pkl\")\n",
    "positive_docs = {}\n",
    "\n",
    "for k, v in positives.items():\n",
    "    positive = v[0]\n",
    "    positive_docs[positive] = k\n",
    "\n",
    "query_sets = [\"train\"]\n",
    "query_datasets = []\n",
    "queries = defaultdict(list)\n",
    "for query_set_idx, query_set in enumerate(query_sets):\n",
    "    qrels = load_pickle(f\"data/cache/{config.dataset}/dataset/query/{query_set}/qrels.pkl\")\n",
    "    for x in qrels:\n",
    "        qidx = x[-2]\n",
    "        tidx = x[-1]\n",
    "        queries[tidx].append((query_set_idx, qidx))\n",
    "    query_datasets.append(QueryDataset(config, query_set))\n",
    "\n",
    "for qidx, positive in positives.items():\n",
    "    # if qidx != 3315:\n",
    "    #     continue\n",
    "    positive = positive[0]\n",
    "    model1_res = model1_result[qidx]\n",
    "    model2_res = model2_result[qidx]\n",
    "    query = query_dataset[qidx][\"query\"][\"input_ids\"]\n",
    "    showcase = False\n",
    "    if positive in model1_res and positive not in model2_res:\n",
    "    # if (positive in model1_res and model1_res.index(positive) < 5) and (positive not in model2_res or model2_res.index(positive) > 5):\n",
    "        text_code1 = text_codes1[positive]\n",
    "        text_code2 = text_codes2[positive]\n",
    "        false_pos_code = text_codes2[model2_res[0]]\n",
    "        string = f\"Model1 wins: {qidx, positive}\"\n",
    "        showcase = True\n",
    "\n",
    "    elif positive in model2_res and positive not in model1_res:\n",
    "        text_code1 = text_codes1[positive]\n",
    "        text_code2 = text_codes2[positive]\n",
    "        false_pos_code = text_codes1[model1_res[0]]\n",
    "        string = f\"Model2 wins: {qidx, positive}\"\n",
    "        showcase = True\n",
    "\n",
    "    if showcase:\n",
    "        print(f\"{string:*^50}\")\n",
    "        print(f\"{'Query': <50}: {t.decode(query, skip_special_tokens=True)}\")\n",
    "        print(f\"{f'Target Anchor ({code_type1})': <50}: {t.decode(text_code1[text_code1 != -1], skip_special_tokens=True)}\")\n",
    "        # print(f\"{f'Target Anchor ({code_type2})': <50}: {t.decode(text_code2[text_code2 != -1], skip_special_tokens=True)}\")\n",
    "        print(f\"{'False Positive Anchor': <50}: {t.decode(false_pos_code[false_pos_code != -1], skip_special_tokens=True)}\")\n",
    "        # print(f\"{f'Target Title ({code_type1})': <50}: {t.decode(text_dataset[positive]['text']['input_ids'][:20], skip_special_tokens=True)}\")\n",
    "        # print(f\"{f'Negative Title  ({code_type2})': <50}: {t.batch_decode(text_dataset[model2_res]['text']['input_ids'][:, :20], skip_special_tokens=True)}\")\n",
    "\n",
    "        all_q = [[] for _ in query_sets]\n",
    "        for query_set_idx, query_idx in queries[positive]:\n",
    "            query = t.decode(query_datasets[query_set_idx][query_idx][\"query\"][\"input_ids\"], skip_special_tokens=True)\n",
    "            all_q[query_set_idx].append(query)\n",
    "        for j, query in enumerate(all_q):\n",
    "            if len(query):\n",
    "                print(f\"{query_sets[j] + ' Queries': <50}: {query}\")        \n",
    "        x = input()\n",
    "        if x == \"s\":\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-06 05:50:00,647] INFO (Dataset) initializing MSMARCO-passage memmap Query dev dataset...\n",
      "[2023-08-06 05:50:00,990] INFO (Dataset) initializing MSMARCO-passage memmap Query train dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean query number for missed queries: 1.424\n",
      "Mean query number for succeeded queries: 1.1515151515151516\n",
      "\n",
      "********Qidx: 1284 Tidx: 7067032********\n",
      "Query               : how many years did william bradford serve as governor of plymouth colony?\n",
      "Anchor              : bradford, plymouth, william, governor, 1657, 1590, colony, leiden, mayflower, leader,\n",
      "Text                : http://en.wikipedia.org/wiki/William_Bradford_(Plymouth_Colony_governor) William Bradford (c.1590 â 1657) was an English Separatist leader in Leiden, Holland and in Plymouth Colony was a signatory to the Mayflower Compact. He served as Plymouth Colony Governor five times covering about thirty years between 1621 and 1657.\n",
      "\n",
      "********Qidx: 3650 Tidx: 7067056********\n",
      "Query               : define preventive\n",
      "Anchor              : preventive, deter, aggression, adjective, obstacle, military, hindering, comparative, most, carried, more, superlative, acting,\n",
      "Text                : Adjective[edit] preventive â(comparative more preventive, superlative most preventive) 1 Preventing, hindering, or acting as an obstacle to. Carried out to deter military aggression.\n",
      "\n",
      "********Qidx: 259 Tidx: 7067274*********\n",
      "Query               : ____________________ is considered the father of modern medicine.\n",
      "Anchor              : medicine, father, modern, hippocrates, considered, true, punishment, believe, false, weegy, illness, because,\n",
      "Text                : TRUE. Hippocrates is considered the father of modern medicine because he did not believe that illness was a punishment inflicted by the gods. True False. Weegy: TRUE. [\n",
      "\n",
      "********Qidx: 2503 Tidx: 7067677********\n",
      "Query               : how much is a cost to run disneyland\n",
      "Anchor              : disney, cost, operating, theme, 355, day, parks, 571, 365, open, average, per, divided, million,\n",
      "Text                : Disney's Theme Parks had an operating cost of 571 million dollars divided by their 11 parks and being open 365 days a year, on average their operating cost per day is around $355,000.\n",
      "\n",
      "********Qidx: 2319 Tidx: 7067891********\n",
      "Query               : do physicians pay for insurance from their salaries?\n",
      "Anchor              : physicians, tricare, health, military, reserve, receive, free, participate, insurance, life, coverage, care, dependents, select,\n",
      "Text                : Active-duty physicians and their dependents will receive free or discounted health care coverage and dental coverage. Physicians in the Reserve and Guard can also participate in TRICARE Reserve Select, which is part of the Military's health care plan. Finally, physicians can receive up to $400,000 in term life insurance coverage for only $29 a month.\n",
      "\n",
      "********Qidx: 5167 Tidx: 7068066********\n",
      "Query               : here there be dragons comic\n",
      "Anchor              : owen, starchild, james, geographica, comic, chronicles, illustrator, author, dragons, imaginarium,\n",
      "Text                : James A. Owen is an American comic book illustrator, publisher and writer. He is known for his creator-owned comic book series Starchild and as the author of The Chronicles of the Imaginarium Geographica novel series, that began with Here, There Be Dragons in 2006. 1 Career.\n"
     ]
    }
   ],
   "source": [
    "# cases for the failed dev queries and the corresponding train queries for a paticular model\n",
    "\n",
    "dataset = \"MSMARCO-passage\"\n",
    "retrieval_result = load_pickle(f\"data/cache/{dataset}/retrieve/AutoTSG/dev/retrieval_result.pkl\")\n",
    "positives = load_pickle(f\"data/cache/{dataset}/dataset/query/dev/positives.pkl\")\n",
    "query_dataset = QueryDataset(config)\n",
    "\n",
    "miss_queries = {}\n",
    "miss_docs = {}\n",
    "\n",
    "success_queries = {}\n",
    "success_docs = {}\n",
    "\n",
    "for k, v in positives.items():\n",
    "    positive = v[0]\n",
    "    res = retrieval_result[k]\n",
    "    if positive not in res:\n",
    "        miss_queries[k] = positive\n",
    "        miss_docs[positive] = k\n",
    "    else:\n",
    "        success_queries[k] = positive\n",
    "        success_docs[positive] = k\n",
    "\n",
    "query_sets = [\"train\"]\n",
    "overlap_miss = defaultdict(list)\n",
    "overlap_success = defaultdict(list)\n",
    "\n",
    "query_datasets = []\n",
    "\n",
    "for query_set_idx, query_set in enumerate(query_sets):\n",
    "    qrels = load_pickle(f\"data/cache/{config.dataset}/dataset/query/{query_set}/qrels.pkl\")\n",
    "    for x in qrels:\n",
    "        qidx = x[-2]\n",
    "        tidx = x[-1]\n",
    "        if tidx in miss_docs:\n",
    "            overlap_miss[tidx].append((query_set_idx, qidx))\n",
    "        elif tidx in success_docs:\n",
    "            overlap_success[tidx].append((query_set_idx, qidx))\n",
    "    query_datasets.append(QueryDataset(config, query_set))\n",
    "\n",
    "print(f\"Mean query number for missed queries: {mean_len(overlap_miss.values())}\\nMean query number for succeeded queries: {mean_len(overlap_success.values())}\")\n",
    "\n",
    "for k, v in miss_queries.items():\n",
    "    # positive = v[0]\n",
    "    positive = v\n",
    "\n",
    "    query = query_dataset[k][\"query\"][\"input_ids\"]\n",
    "    text = text_dataset[positive][\"text\"][\"input_ids\"]\n",
    "    text_code = text_codes[positive]\n",
    "    print(f\"\\n{f'Qidx: {k} Tidx: {positive}':*^40}\")\n",
    "    print(f\"{'Query': <20}: {t.decode(query, skip_special_tokens=True)}\")\n",
    "    # print(f\"{'Target':*^25}\\n{t.decode(text, skip_special_tokens=True)}\")\n",
    "    print(f\"{'Anchor': <20}: {t.decode(text_code[text_code != -1], skip_special_tokens=True)}\")\n",
    "    print(f\"{'Text': <20}: {t.decode(text_dataset[positive]['text']['input_ids'][:100], skip_special_tokens=True)}\")\n",
    "    if positive in overlap_miss:\n",
    "        qindices = overlap_miss[positive]\n",
    "        queries = [[] for _ in query_sets]\n",
    "        for query_set_idx, query_idx in qindices:\n",
    "            queries[query_set_idx].append(t.decode(query_datasets[query_set_idx][query_idx]['query']['input_ids'], skip_special_tokens=True))\n",
    "        for j, query in enumerate(queries):\n",
    "            if len(query):\n",
    "                print(f\"{query_sets[j] + ' Queries': <20}: {query}\")\n",
    "    x = input()\n",
    "    if x == \"s\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false prune plot\n",
    "model = \"BOW_qg\"\n",
    "retrieval_result = load_pickle(f\"data/cache/{config.dataset}/retrieve/{model}/dev/bm25-trie.pkl\")\n",
    "positives = load_pickle(f\"data/cache/{config.dataset}/dataset/query/dev/positives.pkl\")\n",
    "\n",
    "# code_type = \"words_comma_plus_stem\"\n",
    "# code_type = \"title_comma-first\"\n",
    "code_type = \"bm25_comma\"\n",
    "code_length = 34\n",
    "code_tokenizer = \"t5\"\n",
    "wordset = WordSetIndex(save_dir=f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}\", sep_token_id=6)\n",
    "wordset.fit(None)\n",
    "\n",
    "all_missed = 0\n",
    "for k, v in retrieval_result.items():\n",
    "    positive = positives[k][0]\n",
    "    if positive not in v:\n",
    "        all_missed += 1\n",
    "print(all_missed)\n",
    "\n",
    "cutoffs = [1,2,3,4,5]\n",
    "counts = [{\"all_missed\": 0, \"false_prune\": 0, \"wordset_match\": 0, \"decode_step\": i} for i in cutoffs]\n",
    "\n",
    "docs = wordset.docs\n",
    "stopwords = np.array([0, 1, -1])\n",
    "skip_qindices = {}\n",
    "\n",
    "for i, cutoff in enumerate(cutoffs):\n",
    "    false_prune_count = 0\n",
    "    wordset_match_count = 0\n",
    "\n",
    "    for qidx, res in retrieval_result.items():\n",
    "        if qidx in skip_qindices:\n",
    "            continue\n",
    "\n",
    "        positive = positives[qidx][0]\n",
    "        if positive not in res:\n",
    "            gt_doc = docs[positive]\n",
    "            generated_doc = docs[res]\n",
    "            # invalid_at_cutoff = ((gt_doc[cutoff] == -1) * (generated_doc[:, cutoff] == -1)).astype(bool)\n",
    "            # print(invalid_at_cutoff)\n",
    "            \n",
    "            diff_at_cutoff = (gt_doc[cutoff] != generated_doc[:, cutoff])\n",
    "            if diff_at_cutoff.all():\n",
    "                false_prune_count += 1\n",
    "                skip_qindices[qidx] = 1\n",
    "                for gdoc in generated_doc:\n",
    "                    overlap = np.intersect1d(gt_doc, gdoc[:cutoff])  # n\n",
    "                    idx = (overlap[..., None] == stopwords[None, ...]).any(-1)\n",
    "                    overlap = overlap[~idx]                 # <=n\n",
    "                    if len(overlap):\n",
    "                        wordset_match_count += 1\n",
    "                        break\n",
    "\n",
    "        \n",
    "    if i == 0:\n",
    "        counts[i][\"all_missed\"] = all_missed\n",
    "    else:\n",
    "        all_missed = all_missed - counts[i - 1][\"false_prune\"]\n",
    "        counts[i][\"all_missed\"] = all_missed\n",
    "    \n",
    "    counts[i][\"false_prune\"] = false_prune_count\n",
    "    counts[i][\"wordset_match\"] = wordset_match_count\n",
    "    \n",
    "print(counts)\n",
    "data = pd.DataFrame(counts)\n",
    "data.drop(columns=[\"all_missed\"], inplace=True)\n",
    "data = data.melt(id_vars=[\"decode_step\"], value_vars=[\"false_prune\", \"wordset_match\"], value_name=\"count\")\n",
    "ax = sns.barplot(data, x=\"decode_step\", y=\"count\", hue=\"variable\")\n",
    "ax.set_ylabel(\"#Error Query\")\n",
    "ax.set_xlabel(\"Decode Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicated codes and corresponding train queries\n",
    "\n",
    "dup_index = np.argwhere((text_codes == duplicates.loc[0].to_numpy()[:-1]).all(-1))[:, 0]\n",
    "\n",
    "# get train queries\n",
    "queries = [None for _ in range(len(dup_index))]\n",
    "arange = np.arange(len(queries))\n",
    "for x in train_dataset.qrels:\n",
    "    if x[-1] in dup_index:\n",
    "        idx = arange[dup_index == x[-1]][0]\n",
    "        queries[idx] = t.decode(train_dataset.query_datasets[0][x[-2]][\"query\"][\"input_ids\"], skip_special_tokens=True)\n",
    "print(len(dup_index))\n",
    "\n",
    "j = 0\n",
    "with open(\"/share/project/peitian/Data/Adon/Top300k/collection.tsv\") as f:\n",
    "    for i,line in enumerate(f):\n",
    "        if i in dup_index:\n",
    "            for k, v in positives.items():\n",
    "                if idx in v:\n",
    "                    print(f\"{'Dev Query':*^20}\\n{t.decode(query_dataset[k]['query']['input_ids'], skip_special_tokens=True)}\")\n",
    "            print(f\"{'Train Query':*^20}\\n{queries[j]}\")\n",
    "            print(line)\n",
    "            j += 1\n",
    "            x = input()\n",
    "            if x == \"s\":\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases that either res1 or res2 succeeds (different text_codes)\n",
    "\n",
    "model1_result = load_pickle(f\"data/cache/NQ/retrieve/BOW_doct5-miss-doc/dev/best.pkl\")\n",
    "model2_result = load_pickle(f\"data/cache/NQ/retrieve/BOW_doct5-miss-doc/dev/50.pkl\")\n",
    "\n",
    "t = AutoTokenizer.from_pretrained(os.path.join(config.plm_root, code_tokenizer))\n",
    "\n",
    "query_dataset = QueryDataset(config)\n",
    "\n",
    "positives = load_pickle(\"data/cache/NQ/dataset/query/dev/positives.pkl\")\n",
    "positive_docs = {}\n",
    "\n",
    "for k, v in positives.items():\n",
    "    positive = v[0]\n",
    "    positive_docs[positive] = k\n",
    "\n",
    "query_sets = [\"train-sub\", \"doct5-miss-sub\"]\n",
    "query_datasets = []\n",
    "queries = defaultdict(list)\n",
    "for query_set_idx, query_set in enumerate(query_sets):\n",
    "    qrels = load_pickle(f\"data/cache/{config.dataset}/dataset/query/{query_set}/qrels.pkl\")\n",
    "    for x in qrels:\n",
    "        qidx = x[-2]\n",
    "        tidx = x[-1]\n",
    "        queries[tidx].append((query_set_idx, qidx))\n",
    "    query_datasets.append(QueryDataset(config, query_set))\n",
    "\n",
    "code_type1 = \"words_comma\"\n",
    "code_length1 = 34\n",
    "text_codes1 = np.memmap(\n",
    "    f\"data/cache/{config.dataset}/codes/{code_type1}/{code_tokenizer}/{code_length1}/codes.mmp\",\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(len(text_dataset), -1).copy()\n",
    "\n",
    "code_type2 = \"words_comma\"\n",
    "code_length2 = 50\n",
    "text_codes2 = np.memmap(\n",
    "    f\"data/cache/{config.dataset}/codes/{code_type2}/{code_tokenizer}/{code_length2}/codes.mmp\",\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(len(text_dataset), -1).copy()\n",
    "\n",
    "for qidx, positive in positives.items():\n",
    "    positive = positive[0]\n",
    "    model1_res = model1_result[qidx]\n",
    "    model2_res = model2_result[qidx]\n",
    "    query = query_dataset[qidx][\"query\"][\"input_ids\"]\n",
    "    showcase = False\n",
    "    if positive in model1_res and positive not in model2_res:\n",
    "        text_code = text_codes1[positive]\n",
    "        false_pos_code = text_codes2[model2_res[0]]\n",
    "        string = f\"Model1 wins: {qidx, positive}\"\n",
    "        showcase = True\n",
    "    \n",
    "    elif positive in model2_res and positive not in model1_res:\n",
    "        text_code = text_codes2[positive]\n",
    "        false_pos_code = text_codes1[model1_res[0]]\n",
    "        string = f\"Model2 wins: {qidx, positive}\"\n",
    "        showcase = True\n",
    "\n",
    "    if showcase:\n",
    "        print(f\"{string:*^50}\")\n",
    "        print(f\"{'Query': <30}: {t.decode(query, skip_special_tokens=True)}\")\n",
    "        print(f\"{'Target Anchor': <30}: {t.decode(text_code[text_code != -1], skip_special_tokens=True)}\")\n",
    "        string = f\"False Positive Anchor\"\n",
    "        print(f\"{string: <30}: {t.decode(false_pos_code[false_pos_code != -1], skip_special_tokens=True)}\")\n",
    "\n",
    "        all_q = [[] for _ in query_sets]\n",
    "        for query_set_idx, query_idx in queries[positive]:\n",
    "            query = t.decode(query_datasets[query_set_idx][query_idx][\"query\"][\"input_ids\"], skip_special_tokens=True)\n",
    "            all_q[query_set_idx].append(query)\n",
    "        for j, query in enumerate(all_q):\n",
    "            if len(query):\n",
    "                print(f\"{query_sets[j] + ' Queries': <30}: {query}\")        \n",
    "        x = input()\n",
    "        if x == \"s\":\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the document frequency of each code position\n",
    "code_type = \"bm25_comma-sample\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 26\n",
    "wordset = WordSetIndex(save_dir=f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}\", sep_token_id=6)\n",
    "wordset.fit(None)\n",
    "\n",
    "max_word_num = (wordset.docs != -1).sum(-1).max()\n",
    "doc_nums = np.zeros(max_word_num, dtype=np.int32)\n",
    "valid_nums = np.zeros(max_word_num, dtype=np.int32)\n",
    "\n",
    "for i in range(max_word_num):\n",
    "    i_th_position = wordset.docs[:, i]\n",
    "    valid_nums[i] = (i_th_position != -1).sum()\n",
    "    i_th_position = i_th_position[i_th_position != -1]\n",
    "    inverted_lists = wordset.inverted_lists[i_th_position]\n",
    "    doc_nums[i] = sum([len(x) for x in inverted_lists])\n",
    "wordset.inverse_vocab.shape, doc_nums / valid_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new query set based on an existing one\n",
    "\n",
    "# dataset = \"Top300k-filter\"\n",
    "dataset = \"NQ\"\n",
    "ori_query_set = \"nci\"\n",
    "query_set = \"nci-miss\"\n",
    "k = 3\n",
    "\n",
    "try:\n",
    "    qid2idx = load_pickle(f\"data/cache/{dataset}/dataset/query/{ori_query_set}/id2index.pkl\")\n",
    "except FileNotFoundError:\n",
    "    qid2idx = {}\n",
    "    with open(f\"{config.data_root}/{dataset}/queries.{ori_query_set}.tsv\") as f:\n",
    "        for qidx, line in enumerate(tqdm(f, desc=\"Collecting qid2idx\")):\n",
    "            qid = line.split(\"\\t\")[0]\n",
    "            qid2idx[qid] = qidx\n",
    "            \n",
    "tid2idx = load_pickle(f\"data/cache/{dataset}/dataset/text/id2index.pkl\")\n",
    "\n",
    "qindices = []\n",
    "tid2qrels = defaultdict(list)\n",
    "\n",
    "train_positives = load_pickle(f\"data/cache/{dataset}/dataset/query/train/positives.pkl\")\n",
    "train_positives = set([x[0] for x in train_positives.values()])\n",
    "miss_docs = set(range(len(text_dataset))) - train_positives\n",
    "print(f\"number of documents missing in training set: {len(miss_docs)}\")\n",
    "\n",
    "with open(f\"{config.data_root}/{dataset}/qrels.{ori_query_set}.tsv\") as ori_qrel_file, open(f\"{config.data_root}/{dataset}/qrels.{query_set}.tsv\", \"w\") as qrel_file, open(f\"{config.data_root}/{dataset}/queries.{ori_query_set}.tsv\") as ori_query_file, open(f\"{config.data_root}/{dataset}/queries.{query_set}.tsv\", \"w\") as query_file:\n",
    "    for i, line in enumerate(ori_qrel_file):\n",
    "        qid, _, tid, _ = line.strip().split(\"\\t\")\n",
    "        qidx = qid2idx[qid]\n",
    "\n",
    "        # filter out the existing ones\n",
    "        tidx = tid2idx[tid]\n",
    "        if tidx in miss_docs:\n",
    "            tid2qrels[tid].append(line)\n",
    "            qindices.append(qidx)\n",
    "\n",
    "        # # keep the first k elements\n",
    "        # if len(tid2qrels[tid]) >= k:\n",
    "        #     continue\n",
    "        # else:\n",
    "        #     tid2qrels[tid].append(line)\n",
    "        #     qindices.append(qidx)\n",
    "\n",
    "    qindices = set(qindices)\n",
    "    for i, line in enumerate(ori_query_file):\n",
    "        if i in qindices:\n",
    "            query_file.write(line)\n",
    "\n",
    "    for qrels in tid2qrels.values():\n",
    "        for line in qrels:\n",
    "            qrel_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8841823it [01:04, 136960.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# create pseudo-queries from document\n",
    "dataset = \"MSMARCO-passage\"\n",
    "query_set = \"doc\"\n",
    "text_col = [2]\n",
    "query_length = 32\n",
    "\n",
    "with open(f\"{config.data_root}/{dataset}/collection.tsv\") as collection_file, open(f\"{config.data_root}/{dataset}/queries.{query_set}.tsv\", \"w\") as query_file, open(f\"{config.data_root}/{dataset}/qrels.{query_set}.tsv\", \"w\") as qrel_file:\n",
    "    for tidx, line in enumerate(tqdm(collection_file)):\n",
    "        fields = line.split(\"\\t\")\n",
    "        tid = fields[0]\n",
    "        query_fields = [field.strip() for col_idx, field in enumerate(fields) if col_idx in text_col and len(field) > 1]\n",
    "        # maximum number of words\n",
    "        query = \" \".join(query_fields).split(\" \")[:query_length]\n",
    "        query = \" \".join(query)\n",
    "\n",
    "        query_file.write(\"\\t\".join([str(tidx), query]) + \"\\n\")\n",
    "        qrel_file.write(\"\\t\".join([str(tidx), \"0\", tid, \"1\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter query sets with ANCE\n",
    "ori_query_set = \"doct5-5\"\n",
    "query_set = \"doct5-5-filter\"\n",
    "\n",
    "filter_model = \"ANCE\"\n",
    "filter_results = load_pickle(f\"data/cache/{config.dataset}/retrieve/{filter_model}/{ori_query_set}/retrieval_result.pkl\")\n",
    "filter_results = {k: v[:10] for k, v in filter_results.items()}\n",
    "\n",
    "tid2idx = load_pickle(f\"data/cache/{config.dataset}/dataset/text/id2index.pkl\")\n",
    "qid2idx = load_pickle(f\"data/cache/{config.dataset}/dataset/query/{ori_query_set}/id2index.pkl\")\n",
    "\n",
    "with open(f\"{config.data_root}/{config.dataset}/qrels.{ori_query_set}.tsv\") as ori_qrel_file, open(f\"{config.data_root}/{config.dataset}/queries.{ori_query_set}.tsv\") as ori_query_file, open(f\"{config.data_root}/{config.dataset}/qrels.{query_set}.tsv\", \"w\") as qrel_file, open(f\"{config.data_root}/{config.dataset}/queries.{query_set}.tsv\", \"w\") as query_file:\n",
    "    qindices = set()\n",
    "    for i, line in enumerate(ori_qrel_file):\n",
    "        qid, _, tid, _ = line.strip().split(\"\\t\")\n",
    "        qidx = qid2idx[qid]\n",
    "        tidx = tid2idx[tid]\n",
    "        if tidx in filter_results[qidx]:\n",
    "            qrel_file.write(line)\n",
    "            qindices.add(qidx)\n",
    "    for i, line in enumerate(ori_query_file):\n",
    "        if i in qindices:\n",
    "            query_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter MSMARCO Top300k\n",
    "\n",
    "# filter_indices = []\n",
    "# for dup_idx in dup_indices:\n",
    "#     text = t.decode(text_dataset[dup_idx][\"text\"][\"input_ids\"], skip_special_tokens=True)\n",
    "#     text_length = len(text.split(\" \"))\n",
    "#     if text_length < 100:\n",
    "#         filter_indices.append(dup_idx)\n",
    "\n",
    "filter_indices = set(np.argwhere(df.duplicated().to_numpy())[:, 0].tolist())\n",
    "\n",
    "with open(\"/share/project/peitian/Data/Adon/Top300k/collection.tsv\") as ori_collection, open(\"/share/project/peitian/Data/Adon/Top300k/qrels.train.tsv\") as ori_train_qrels, open(\"/share/project/peitian/Data/Adon/Top300k/qrels.dev.tsv\") as ori_dev_qrels, open(\"/share/project/peitian/Data/Adon/Top300k-filter/collection.tsv\", \"w\") as collection, open(\"/share/project/peitian/Data/Adon/Top300k-filter/qrels.train.tsv\", \"w\") as train_qrels, open(\"/share/project/peitian/Data/Adon/Top300k-filter/qrels.dev.tsv\", \"w\") as dev_qrels:\n",
    "    # shutil.copy(\"/share/project/peitian/Data/Adon/Top300k/queries.train.tsv\", \"/share/project/peitian/Data/Adon/Top300k-filter/queries.train.tsv\")\n",
    "    # shutil.copy(\"/share/project/peitian/Data/Adon/Top300k/queries.dev.tsv\", \"/share/project/peitian/Data/Adon/Top300k-filter/queries.dev.tsv\")\n",
    "\n",
    "    for i, line in enumerate(ori_collection):\n",
    "        if i not in filter_indices:\n",
    "            collection.write(line)\n",
    "\n",
    "    tid2idx = load_pickle(\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/Top300k/dataset/text/id2index.pkl\")\n",
    "    for line in ori_train_qrels:\n",
    "        qid, _, tid, _ = line.strip().split()\n",
    "        tidx = tid2idx[tid]\n",
    "        if tidx in filter_indices:\n",
    "            continue\n",
    "        train_qrels.write(line)\n",
    "    \n",
    "    for line in ori_dev_qrels:\n",
    "        qid, _, tid, _ = line.strip().split()\n",
    "        tidx = tid2idx[tid]\n",
    "        if tidx in filter_indices:\n",
    "            continue\n",
    "        dev_qrels.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert yujia DSI code to my format\n",
    "\n",
    "tid2idx = load_pickle(\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/Top300k-filter/dataset/text/id2index.pkl\")\n",
    "\n",
    "code_type = \"DSI-semantic\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 10\n",
    "\n",
    "t = AutoTokenizer.from_pretrained(os.path.join(config.plm_root, code_tokenizer))\n",
    "text_codes = np.memmap(\n",
    "    makedirs(f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=(len(text_dataset), code_length)\n",
    ")\n",
    "text_codes[:, 0] = 0\n",
    "text_codes[:, 1:] = -1\n",
    "\n",
    "count = 0\n",
    "with open(\"/share/project/webbrain-zhouyujia/transfer/data/encoded_docid/t5_semantic_structured_top_300k.txt\") as f:\n",
    "    index = 0\n",
    "    for line in tqdm(f):\n",
    "        tid, code = line.strip().split()\n",
    "        tid = tid[1:-1].upper()\n",
    "        if tid in tid2idx:\n",
    "            count += 1\n",
    "            code = [int(x) for x in code.split(\",\")]\n",
    "            text_codes[index, 1:len(code)+1] = code\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert yujia ultron code to my format\n",
    "\n",
    "dataset = \"Rand300k-filter\"\n",
    "tid2idx = load_pickle(f\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/{dataset}/dataset/text/id2index.pkl\")\n",
    "\n",
    "code_type = \"ultron\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 34\n",
    "\n",
    "config = Config()\n",
    "with initialize(version_base=None, config_path=\"../data/config/\"):\n",
    "    overrides = [\n",
    "        f\"base={dataset}\",\n",
    "    ]\n",
    "    hydra_config = compose(config_name=\"_example\", overrides=overrides)\n",
    "    config._from_hydra(hydra_config)\n",
    "text_dataset = TextDataset(config)\n",
    "# t = AutoTokenizer.from_pretrained(os.path.join(config.plm_root, code_tokenizer))\n",
    "text_codes = np.memmap(\n",
    "    makedirs(f\"data/cache/{dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=(len(text_dataset), code_length)\n",
    ")\n",
    "text_codes[:, 0] = 0\n",
    "text_codes[:, 1:] = -1\n",
    "\n",
    "count = 0\n",
    "with open(\"/share/project/webbrain-zhouyujia/transfer/data/encoded_docid/t5_url_title_rand_300k.txt\") as f:\n",
    "    index = 0\n",
    "    for line in tqdm(f):\n",
    "        tid, code = line.strip().split()\n",
    "        tid = tid[1:-1].upper()\n",
    "        if tid in tid2idx:\n",
    "            count += 1\n",
    "            code = [int(x) for x in code.split(\",\")]\n",
    "            if len(code) > 33:\n",
    "                code = code[:33]\n",
    "                code[-1] = 1\n",
    "            text_codes[index, 1:len(code)+1] = code\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert yujia doct5 to my format\n",
    "\n",
    "tid2idx = load_pickle(\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/Top300k-filter/dataset/text/id2index.pkl\")\n",
    "\n",
    "with open(\"/share/project/webbrain-zhouyujia/transfer/data/msmarco-data/fake_query_10_all.txt\") as f, open(\"/share/project/peitian/Data/Adon/Top300k-filter/queries.doct5.tsv\", \"w\") as query_file, open(\"/share/project/peitian/Data/Adon/Top300k-filter/qrels.doct5.tsv\", \"w\") as qrel_file:\n",
    "    for i, line in enumerate(tqdm(f)):\n",
    "        tid, query = line.split(\"\\t\")\n",
    "        tid = tid[1:-1].upper()\n",
    "\n",
    "        qid = str(i)\n",
    "        if tid in tid2idx:\n",
    "            query_file.write(\"\\t\".join([qid, query]))\n",
    "            qrel_file.write(\"\\t\".join([qid, \"0\", tid, \"1\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ultron code\n",
    "code_type = \"ultron\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 34\n",
    "text_codes_rand = np.memmap(\n",
    "    makedirs(f\"data/cache/Rand300k-filter/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(-1, code_length).copy()\n",
    "text_codes_top = np.memmap(\n",
    "    makedirs(f\"data/cache/Top300k-filter/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(-1, code_length).copy()\n",
    "\n",
    "print(text_codes_rand.shape, text_codes_top.shape)\n",
    "\n",
    "config = Config()\n",
    "with initialize(version_base=None, config_path=\"../data/config/\"):\n",
    "    overrides = [\n",
    "        f\"base=MS600k\",\n",
    "    ]\n",
    "    hydra_config = compose(config_name=\"_example\", overrides=overrides)\n",
    "    config._from_hydra(hydra_config)\n",
    "text_dataset = TextDataset(config)\n",
    "text_codes = np.memmap(\n",
    "    makedirs(f\"data/cache/MS600k/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=(len(text_dataset), code_length)\n",
    ")\n",
    "text_codes[:, 0] = 0\n",
    "text_codes[:, 1:] = -1\n",
    "\n",
    "text_codes[:len(text_codes_rand)] = text_codes_rand\n",
    "text_codes[len(text_codes_rand):] = text_codes_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split code from MS600k\n",
    "\n",
    "# code_type = \"ANCE_hier\"\n",
    "code_type = \"NCI-bias\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 10\n",
    "# code_type_split = \"ANCE_hier_600k\"\n",
    "code_type_split = \"NCI_600k-bias\"\n",
    "\n",
    "config = Config()\n",
    "with initialize(version_base=None, config_path=\"../data/config/\"):\n",
    "    overrides = [\n",
    "        f\"base=MS600k\",\n",
    "    ]\n",
    "    hydra_config = compose(config_name=\"_example\", overrides=overrides)\n",
    "    config._from_hydra(hydra_config)\n",
    "text_dataset = TextDataset(config)\n",
    "\n",
    "text_codes = np.memmap(\n",
    "    makedirs(f\"data/cache/MS600k/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32,\n",
    ").reshape(text_dataset.text_num, code_length).copy()\n",
    "\n",
    "config = Config()\n",
    "with initialize(version_base=None, config_path=\"../data/config/\"):\n",
    "    overrides = [\n",
    "        f\"base=Rand300k-filter\",\n",
    "    ]\n",
    "    hydra_config = compose(config_name=\"_example\", overrides=overrides)\n",
    "    config._from_hydra(hydra_config)\n",
    "rand_text_dataset = TextDataset(config)\n",
    "\n",
    "text_codes_rand = np.memmap(\n",
    "    makedirs(f\"data/cache/Rand300k-filter/codes/{code_type_split}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=(rand_text_dataset.text_num, code_length),\n",
    ")\n",
    "text_codes_top = np.memmap(\n",
    "    makedirs(f\"data/cache/Top300k-filter/codes/{code_type_split}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=(text_dataset.text_num - text_codes_rand.shape[0], code_length)    \n",
    ")\n",
    "\n",
    "text_codes_rand[:] = text_codes[:rand_text_dataset.text_num]\n",
    "text_codes_top[:] = text_codes[rand_text_dataset.text_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split some queries\n",
    "\n",
    "ori_dataset = \"NQ\"\n",
    "dataset = \"NQ-50k-seen\"\n",
    "query_set = \"doct5\"\n",
    "\n",
    "tid2idx = load_pickle(f\"data/cache/{dataset}/dataset/text/id2index.pkl\")\n",
    "qids = set()\n",
    "\n",
    "with \\\n",
    "    open(f\"{config.data_root}/{ori_dataset}/qrels.{query_set}.tsv\") as ori_qrel_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/qrels.{query_set}.tsv\", \"w\") as qrel_file, \\\n",
    "    open(f\"{config.data_root}/{ori_dataset}/queries.{query_set}.tsv\") as ori_query_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/queries.{query_set}.tsv\", \"w\") as query_file:\n",
    "    for i, line in enumerate(ori_qrel_file):\n",
    "        qid, _, tid, _ = line.strip().split(\"\\t\")\n",
    "        if tid in tid2idx:\n",
    "            qids.add(qid)\n",
    "            qrel_file.write(line)\n",
    "    \n",
    "    for line in ori_query_file:\n",
    "        qid, query = line.split(\"\\t\")\n",
    "        if qid in qids:\n",
    "            query_file.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert t5 code to bert code\n",
    "\n",
    "code_type = \"chat\"\n",
    "# code_type = \"words-weight\"\n",
    "code_tokenizer = \"t5\"\n",
    "code_length = 50\n",
    "# code_length = 34\n",
    "\n",
    "text_codes = np.memmap(\n",
    "    f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\",\n",
    "    mode=\"r\",\n",
    "    dtype=np.int32\n",
    ").reshape(len(text_dataset), -1).copy()\n",
    "\n",
    "new_code_tokenizer = \"bert\"\n",
    "new_t = AutoTokenizer.from_pretrained(os.path.join(config.plm_root, new_code_tokenizer))\n",
    "\n",
    "new_text_codes = []\n",
    "max_length = 0\n",
    "for text_code in tqdm(text_codes):\n",
    "    text_code = text_code[text_code != -1]\n",
    "    decoded = t.decode(text_code, skip_special_tokens=True)\n",
    "    encoded = new_t.encode(decoded, padding=False)\n",
    "    if new_code_tokenizer == \"bert\":\n",
    "        encoded = new_t.encode(decoded, padding=False)[1:]\n",
    "    new_text_codes.append(encoded)\n",
    "    if len(encoded) > max_length:\n",
    "        max_length = len(encoded)\n",
    "\n",
    "# plus one for the leading padding token\n",
    "mmp_path = f\"data/cache/{config.dataset}/codes/{code_type}/{new_code_tokenizer}/{max_length + 1}/codes.mmp\"\n",
    "makedirs(mmp_path)\n",
    "new_codes_mmp = np.memmap(\n",
    "    mmp_path,\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=(len(text_dataset), max_length + 1)\n",
    ")\n",
    "new_codes_mmp[:,1:] = -1\n",
    "\n",
    "for qrel_idx, code in enumerate(tqdm(new_text_codes)):\n",
    "    new_codes_mmp[qrel_idx, 1:len(code) + 1] = code\n",
    "\n",
    "print(f\"saving at {mmp_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder words-comma in ascending df\n",
    "\n",
    "new_docs = np.zeros_like(wordset.docs) - 1\n",
    "for i, doc in enumerate(tqdm(wordset.docs)):\n",
    "    doc = doc[doc != -1]\n",
    "    inverted_lists = wordset.inverted_lists[doc]\n",
    "    dfs = [len(x) for x in inverted_lists]\n",
    "\n",
    "    new_doc = sorted(zip(doc, dfs), key=lambda x: x[1])\n",
    "    new_doc = [x[0] for x in new_doc]\n",
    "    new_docs[i, :len(new_doc)] = new_doc\n",
    "\n",
    "code_type = \"words-comma-df\"\n",
    "path = f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"\n",
    "makedirs(path)\n",
    "\n",
    "print(f\"saving at {path}...\")\n",
    "\n",
    "new_text_codes = np.memmap(\n",
    "    path,\n",
    "    mode=\"w+\",\n",
    "    dtype=np.int32,\n",
    "    shape=text_codes.shape\n",
    ")\n",
    "new_text_codes[:, 0] = text_codes[:, 0]\n",
    "new_text_codes[:, 1:] = -1\n",
    "\n",
    "sep_token_id = int(text_codes[0][text_codes[0] != -1][-1])\n",
    "\n",
    "for i, new_doc in enumerate(tqdm(new_docs)):\n",
    "    new_doc = new_doc[new_doc != -1]\n",
    "    tokens = wordset.inverse_vocab[new_doc].reshape(-1)\n",
    "    tokens = tokens[tokens != -1].tolist()\n",
    "    tokens.append(sep_token_id)\n",
    "    new_text_codes[i, 1: len(tokens) + 1] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new documents to Top300k-filter\n",
    "tid2index = load_pickle(\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/Top300k-filter/dataset/text/id2index.pkl\")\n",
    "\n",
    "with \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/collection.backup.tsv\") as f, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/collection.tsv\", \"w\") as g, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/qrels.dev.backup.tsv\") as dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/qrels.dev.tsv\", \"w\") as new_dev_qrel:\n",
    "\n",
    "    tids = []\n",
    "    for i, line in enumerate(f):\n",
    "        tid = line.split(\"\\t\")[0]\n",
    "        if tid not in tid2index:\n",
    "            tids.append(tid)\n",
    "            g.write(line)\n",
    "    \n",
    "    for line in dev_qrel:\n",
    "        qid, _, tid, _ = line.strip().split()\n",
    "        if tid in tids:\n",
    "            new_dev_qrel.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid2index = load_pickle(\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/Rand300k-filter/dataset/text/id2index.pkl\")\n",
    "tindex2id = {v: k for k, v in tid2index.items()}\n",
    "dup_tids = {tindex2id[x] for x in dup_indices}\n",
    "\n",
    "# filtered 300k\n",
    "with \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/collection.backup.tsv\") as collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/collection.tsv\", \"w\") as new_collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/qrels.dev.backup.tsv\") as dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/qrels.dev.tsv\", \"w\") as new_dev_qrel:\n",
    "\n",
    "    tids = []\n",
    "    for i, line in enumerate(collection_file):\n",
    "        tid = line.split(\"\\t\")[0]\n",
    "        if tid in tid2index and tid not in dup_tids:\n",
    "            tids.append(tid)\n",
    "            new_collection_file.write(line)\n",
    "    \n",
    "    for line in dev_qrel:\n",
    "        qid, _, tid, _ = line.strip().split()\n",
    "        if tid in tids:\n",
    "            new_dev_qrel.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tid2index = load_pickle(\"data/cache/Rand300k-filter/dataset/text/id2index.pkl\")\n",
    "tindex2id = {v: k for k, v in tid2index.items()}\n",
    "\n",
    "positives = load_pickle(\"data/cache/Rand300k-filter/dataset/query/dev/positives.pkl\")\n",
    "positives = set([v[0] for v in positives.values()])\n",
    "positive_tids = [tindex2id[x] for x in positives]\n",
    "complement = sample(list([k for k, v in tid2index.items() if v not in positives]), 100000 - len(positives))\n",
    "tids = set(positive_tids + complement)\n",
    "print(len(tids))\n",
    "\n",
    "# filtered 300k\n",
    "with \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/collection.backup.tsv\") as collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand100k-filter/collection.tsv\", \"w\") as new_collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/qrels.dev.backup.tsv\") as dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand100k-filter/qrels.dev.tsv\", \"w\") as new_dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/queries.dev.backup.tsv\") as dev_query, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand100k-filter/queries.dev.tsv\", \"w\") as new_dev_query:\n",
    "\n",
    "    for i, line in enumerate(collection_file):\n",
    "        tid = line.split(\"\\t\")[0]\n",
    "        if tid in tids:\n",
    "            tids.add(tid)\n",
    "            new_collection_file.write(line)\n",
    "\n",
    "    qids = set()\n",
    "    for line in dev_qrel:\n",
    "        qid, _, tid, _ = line.strip().split()\n",
    "        if tid in tids:\n",
    "            new_dev_qrel.write(line)\n",
    "            qids.add(qid)\n",
    "\n",
    "    for line in dev_query:\n",
    "        qid = line.strip().split()[0]\n",
    "        if qid in qids:\n",
    "            new_dev_query.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/collection.tsv\") as rand_collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Top300k-filter/collection.tsv\") as top_collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/MS600k/collection.tsv\", \"w\") as new_collection_file, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/qrels.dev.tsv\") as rand_dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Top300k-filter/qrels.dev.tsv\") as top_dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/MS600k/qrels.dev.tsv\", \"w\") as new_dev_qrel, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Rand300k-filter/queries.dev.tsv\") as rand_dev_query, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/Top300k-filter/queries.dev.tsv\") as top_dev_query, \\\n",
    "    open(\"/share/project/peitian/Data/Adon/MS600k/queries.dev.tsv\", \"w\") as new_dev_query:\n",
    "\n",
    "    rand_tids = []\n",
    "    for line in rand_collection_file:\n",
    "        tid = line.split(\"\\t\")[0]\n",
    "        rand_tids.append(tid)\n",
    "        new_collection_file.write(line)\n",
    "    \n",
    "    top_tids = []\n",
    "    for line in top_collection_file:\n",
    "        tid = line.split(\"\\t\")[0]\n",
    "        top_tids.append(tid)\n",
    "        new_collection_file.write(line)\n",
    "\n",
    "    assert len(set(rand_tids).intersection(set(top_tids))) == 0\n",
    "\n",
    "    for line in rand_dev_query:\n",
    "        new_dev_query.write(line)\n",
    "    for line in top_dev_query:\n",
    "        new_dev_query.write(line)\n",
    "    for line in rand_dev_qrel:\n",
    "        new_dev_qrel.write(line)\n",
    "    for line in top_dev_qrel:\n",
    "        new_dev_qrel.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split NQ to Seen and Unseen for evaluating performance of adding new documents\n",
    "# d0_tindices, d0_train_positives, d0_dev_positives, d0_test_positives (3915)\n",
    "# d1_tindices, d1_test_positives (3915)\n",
    "\n",
    "train_positives = load_pickle(f\"data/cache/{config.dataset}/dataset/query/train/positives.pkl\")\n",
    "test_positives = load_pickle(f\"data/cache/{config.dataset}/dataset/query/dev/positives.pkl\")\n",
    "\n",
    "tidx_2_train_query = defaultdict(list)\n",
    "for k, v in train_positives.items():\n",
    "    tidx_2_train_query[v[0]].append(k)\n",
    "tindices_with_train_query = set([x[0] for x in train_positives.values()])\n",
    "\n",
    "tidx_2_test_query = defaultdict(list)\n",
    "for k, v in test_positives.items():\n",
    "    tidx_2_test_query[v[0]].append(k)\n",
    "tindices_with_test_query = set([x[0] for x in test_positives.values()])\n",
    "\n",
    "tindices = set(range(len(text_dataset)))\n",
    "\n",
    "test_qindices = set(test_positives.keys())\n",
    "d1_test_positives = {}\n",
    "# unseen documents on the entire training set\n",
    "d1_tindices = tindices - tindices_with_train_query\n",
    "for tidx in d1_tindices:\n",
    "    qindices = tidx_2_test_query[tidx]\n",
    "    for qidx in qindices:\n",
    "        d1_test_positives[qidx] = [tidx]\n",
    "\n",
    "# sampled queries\n",
    "d1_test_candidates = test_qindices - set(d1_test_positives.keys())\n",
    "# only keep the queries with 1 relevant document\n",
    "d1_test_qindices = set()\n",
    "for qidx in d1_test_candidates:\n",
    "    if len(tidx_2_test_query[test_positives[qidx][0]]) == 1:\n",
    "        d1_test_qindices.add(qidx)\n",
    "d1_test_candidates = random.sample(list(d1_test_qindices), 3915 - len(d1_test_positives))\n",
    "for qidx in d1_test_candidates:\n",
    "    d1_test_positives[qidx] = test_positives[qidx]\n",
    "\n",
    "d1_tindices = set([x[0] for x in d1_test_positives.values()])\n",
    "# sampled documents relevant to some train queries\n",
    "d1_candidates = tindices_with_train_query - tindices_with_test_query\n",
    "d1_tindices.update(random.sample(list(d1_candidates), 50000 - len(d1_tindices)))\n",
    "\n",
    "d0_test_positives = {}\n",
    "d0_test_tindices = tindices_with_test_query - d1_tindices\n",
    "for tidx in d0_test_tindices:\n",
    "    qindices = tidx_2_test_query[tidx]\n",
    "    for qidx in qindices:\n",
    "        d0_test_positives[qidx] = [tidx]\n",
    "\n",
    "# sanity\n",
    "d0_tindices = tindices - d1_tindices\n",
    "for tidx in d0_tindices:\n",
    "    assert tidx in tidx_2_train_query\n",
    "assert len(d0_test_positives) == 3915\n",
    "assert d0_test_tindices - d0_tindices == set()\n",
    "\n",
    "d0_train_positives = {}\n",
    "d0_dev_positives = {}\n",
    "\n",
    "d0_qindices = set()\n",
    "d0_tindices_with_multiple_rel = set()\n",
    "for tidx in d0_tindices:\n",
    "    qindices = tidx_2_train_query[tidx]\n",
    "    d0_qindices.update(qindices)\n",
    "    if len(qindices) > 1:\n",
    "        d0_tindices_with_multiple_rel.add(tidx)\n",
    "\n",
    "d0_dev_tindices = random.sample(list(d0_tindices_with_multiple_rel), 3000)\n",
    "for tidx in d0_dev_tindices:\n",
    "    qindices = tidx_2_train_query[tidx]\n",
    "    assert len(qindices) > 1\n",
    "    dev_qidx = random.sample(qindices, 1)[0]\n",
    "    d0_dev_positives[dev_qidx] = [tidx]\n",
    "    d0_qindices.remove(dev_qidx)\n",
    "\n",
    "for qidx in d0_qindices:\n",
    "    d0_train_positives[qidx] = train_positives[qidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidx2id = {v: k for k, v in load_pickle(f\"data/cache/{config.dataset}/dataset/text/id2index.pkl\").items()}\n",
    "# 50k\n",
    "d0_dataset = \"NQ-50k-seen\"\n",
    "# the remaining 50k\n",
    "d1_dataset = \"NQ-50k-unseen\"\n",
    "\n",
    "train_qidx2id = {v: k for k, v in load_pickle(f\"data/cache/{config.dataset}/dataset/query/train/id2index.pkl\").items()}\n",
    "dev_qidx2id = {v: k for k, v in load_pickle(f\"data/cache/{config.dataset}/dataset/query/dev/id2index.pkl\").items()}\n",
    "\n",
    "os.makedirs(f\"{config.data_root}/{d0_dataset}\", exist_ok=True)\n",
    "os.makedirs(f\"{config.data_root}/{d1_dataset}\", exist_ok=True)\n",
    "\n",
    "# with open(f\"{config.data_root}/{config.dataset}/collection.tsv\") as collection_file, \\\n",
    "#     open(f\"{config.data_root}/{config.dataset}/queries.train.tsv\") as train_query_file, \\\n",
    "#     open(f\"{config.data_root}/{config.dataset}/queries.dev.tsv\") as dev_query_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/collection.tsv\", \"w\") as d0_collection_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/queries.train.tsv\", \"w\") as d0_train_query_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/queries.dev.tsv\", \"w\") as d0_dev_query_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/queries.test.tsv\", \"w\") as d0_test_query_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/qrels.train.tsv\", \"w\") as d0_train_qrel_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/qrels.dev.tsv\", \"w\") as d0_dev_qrel_file, \\\n",
    "#     open(f\"{config.data_root}/{d0_dataset}/qrels.test.tsv\", \"w\") as d0_test_qrel_file, \\\n",
    "#     open(f\"{config.data_root}/{d1_dataset}/collection.tsv\", \"w\") as d1_collection_file, \\\n",
    "#     open(f\"{config.data_root}/{d1_dataset}/queries.dev.tsv\", \"w\") as d1_test_query_file, \\\n",
    "#     open(f\"{config.data_root}/{d1_dataset}/qrels.dev.tsv\", \"w\") as d1_test_qrel_file:\n",
    "\n",
    "#     for tidx, line in enumerate(tqdm(collection_file)):\n",
    "#         if tidx in d0_tindices:\n",
    "#             d0_collection_file.write(line)\n",
    "#         if tidx in d1_tindices:\n",
    "#             d1_collection_file.write(line)\n",
    "    \n",
    "#     for qidx, line in enumerate(tqdm(train_query_file)):\n",
    "#         if qidx in d0_train_positives:\n",
    "#             positive = d0_train_positives[qidx][0]\n",
    "#             d0_train_query_file.write(line)\n",
    "#             d0_train_qrel_file.write(\"\\t\".join([train_qidx2id[qidx], \"0\", tidx2id[positive], \"1\"]) + \"\\n\")\n",
    "#         elif qidx in d0_dev_positives:\n",
    "#             positive = d0_dev_positives[qidx][0]\n",
    "#             d0_dev_query_file.write(line)\n",
    "#             d0_dev_qrel_file.write(\"\\t\".join([train_qidx2id[qidx], \"0\", tidx2id[positive], \"1\"]) + \"\\n\")\n",
    "\n",
    "#     for qidx, line in enumerate(tqdm(dev_query_file)):\n",
    "#         if qidx in d0_test_positives:\n",
    "#             positive = d0_test_positives[qidx][0]\n",
    "#             d0_test_query_file.write(line)\n",
    "#             d0_test_qrel_file.write(\"\\t\".join([dev_qidx2id[qidx], \"0\", tidx2id[positive], \"1\"]) + \"\\n\")\n",
    "#         if qidx in d1_test_positives:\n",
    "#             positive = d1_test_positives[qidx][0]\n",
    "#             d1_test_query_file.write(line)\n",
    "#             d1_test_qrel_file.write(\"\\t\".join([dev_qidx2id[qidx], \"0\", tidx2id[positive], \"1\"]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_types = [\"ANCE_hier\", \"words_comma\", \"words_comma_plus_stem\", \"title\"]\n",
    "code_types = [\"id\"]\n",
    "code_lengths = [8]\n",
    "code_tokenizer = \"t5\"\n",
    "\n",
    "for code_type, code_length in zip(code_types, code_lengths):\n",
    "    text_codes = np.memmap(\n",
    "        f\"data/cache/{config.dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\",\n",
    "        mode=\"r\",\n",
    "        dtype=np.int32\n",
    "    ).reshape(len(text_dataset), code_length).copy()\n",
    "\n",
    "    d0_text_codes = np.memmap(\n",
    "        makedirs(f\"data/cache/{d0_dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "        mode=\"w+\",\n",
    "        dtype=np.int32,\n",
    "        shape=(len(d0_tindices), code_length)\n",
    "    )\n",
    "    d1_text_codes = np.memmap(\n",
    "        makedirs(f\"data/cache/{d1_dataset}/codes/{code_type}/{code_tokenizer}/{code_length}/codes.mmp\"),\n",
    "        mode=\"w+\",\n",
    "        dtype=np.int32,\n",
    "        shape=(len(d1_tindices), code_length)\n",
    "    )\n",
    "    counts = [0 for _ in range(4)]\n",
    "    for tidx, text_code in enumerate(text_codes):\n",
    "        if tidx in d0_tindices:\n",
    "            d0_text_codes[counts[0]] = text_code\n",
    "            counts[0] += 1\n",
    "        if tidx in d1_tindices:\n",
    "            d1_text_codes[counts[1]] = text_code\n",
    "            counts[1] += 1\n",
    "    assert all([counts[i] == len(eval(f\"d{i}_tindices\")) for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case for generation likelihood\n",
    "model = AM.from_pretrained(\"/share/project/peitian/Code/Uni-Retriever/src/data/cache/NQ/ckpts/BOW_qg/first+random3+em\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = default_collate([query_dataset[3315][\"query\"]])\n",
    "\n",
    "text_code = [0]\n",
    "# for x in \"executive office of president\".split(\" \"):\n",
    "# for x in \"white house communication director\".split(\" \"):\n",
    "for x in \"cristeta comerford executive chef\".split(\" \"):\n",
    "# for x in \"executive chef white house\".split(\" \"):\n",
    "# for x in \"white house executive chef\".split(\" \"):\n",
    "    text_code += t.encode(x, add_special_tokens=False) + [6]\n",
    "text_code += [1]\n",
    "text_code = default_collate([text_code])\n",
    "\n",
    "# text_code = default_collate([text_codes[108361].astype(np.int64)])\n",
    "\n",
    "text_code[text_code == -1] = 0\n",
    "display(t.decode(text_code[0]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model.plm(**query, decoder_input_ids=text_code).logits\n",
    "    logits = torch.log_softmax(logits, dim=-1)\n",
    "    logits = logits.gather(dim=-1, index=text_code[:, 1:, None])[0,:,0]\n",
    "    # logits[4] = logits[4] - 4\n",
    "    cum = logits.cumsum(dim=-1).numpy().round(3)\n",
    "    tokens = t.convert_ids_to_tokens(text_code[0])[1:]\n",
    "list(zip(cum, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interleave two query sets to form a unified one with all documents having the same number of queries\n",
    "\n",
    "k = 5\n",
    "dataset = \"NQ\"\n",
    "query_set = \"pad5\"\n",
    "main_query_set = \"train\"\n",
    "sup_query_set = \"doct5\"\n",
    "\n",
    "main_qid2idx = load_pickle(f\"data/cache/{dataset}/dataset/query/{main_query_set}/id2index.pkl\")\n",
    "sup_qid2idx = load_pickle(f\"data/cache/{dataset}/dataset/query/{sup_query_set}/id2index.pkl\")\n",
    "tid2idx = load_pickle(f\"data/cache/{dataset}/dataset/text/id2index.pkl\")\n",
    "\n",
    "with \\\n",
    "    open(f\"{config.data_root}/{dataset}/qrels.{main_query_set}.tsv\") as main_qrel_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/qrels.{sup_query_set}.tsv\") as sup_qrel_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/qrels.{query_set}.tsv\", \"w\") as qrel_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/queries.{main_query_set}.tsv\") as main_query_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/queries.{sup_query_set}.tsv\") as sup_query_file, \\\n",
    "    open(f\"{config.data_root}/{dataset}/queries.{query_set}.tsv\", \"w\") as query_file:\n",
    "\n",
    "    new_qid = 0\n",
    "    # map qidx to a new query id\n",
    "    qidx2newid = {}\n",
    "    qid2idx = load_pickle(f\"data/cache/{dataset}/dataset/query/{main_query_set}/id2index.pkl\")\n",
    "    main_tid2qidx = defaultdict(list)\n",
    "    for i, line in enumerate(main_qrel_file):\n",
    "        qid, _, tid, _ = line.strip().split(\"\\t\")\n",
    "        # keep the first k elements\n",
    "        if len(main_tid2qidx[tid]) < k:\n",
    "            qidx = qid2idx[qid]\n",
    "            main_tid2qidx[tid].append(qidx)\n",
    "            qidx2newid[qidx] = new_qid\n",
    "            new_qid += 1\n",
    "\n",
    "    # slice main queries\n",
    "    for qidx, line in enumerate(main_query_file):\n",
    "        if qidx in qidx2newid:\n",
    "            qid, query = line.split(\"\\t\")\n",
    "            qid = qidx2newid[qidx]\n",
    "            query_file.write(\"\\t\".join([str(qid), query]))\n",
    "\n",
    "    # store qrel using new_qid\n",
    "    for tid, qindices in main_tid2qidx.items():\n",
    "        for qidx in qindices:\n",
    "            qid = qidx2newid[qidx]\n",
    "            qrel_file.write(\"\\t\".join([str(qid), \"0\", tid, \"1\"]) + '\\n')\n",
    "\n",
    "    qidx2newid = {}\n",
    "    sup_tid2qidx = defaultdict(list)\n",
    "    qid2idx = load_pickle(f\"data/cache/{dataset}/dataset/query/{sup_query_set}/id2index.pkl\")\n",
    "    for i, line in enumerate(sup_qrel_file):\n",
    "        qid, _, tid, _ = line.strip().split(\"\\t\")\n",
    "        # keep the first k elements\n",
    "        # pad to k\n",
    "        if len(main_tid2qidx[tid]) + len(sup_tid2qidx[tid]) < k:\n",
    "            qidx = qid2idx[qid]\n",
    "            sup_tid2qidx[tid].append(qidx)\n",
    "            qidx2newid[qidx] = new_qid\n",
    "            new_qid += 1\n",
    "    \n",
    "    # slice sup queries\n",
    "    for qidx, line in enumerate(sup_query_file):\n",
    "        if qidx in qidx2newid:\n",
    "            qid, query = line.split(\"\\t\")\n",
    "            qid = qidx2newid[qidx]\n",
    "            query_file.write(\"\\t\".join([str(qid), query]))\n",
    "\n",
    "    # store qrel using new_qid\n",
    "    for tid, qindices in sup_tid2qidx.items():\n",
    "        for qidx in qindices:\n",
    "            qid = qidx2newid[qidx]\n",
    "            qrel_file.write(\"\\t\".join([str(qid), \"0\", tid, \"1\"]) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "778a5a6b0df35a46498564cf16af2e5ec016022ef7dc9d5934de67fcb1f6bfb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
